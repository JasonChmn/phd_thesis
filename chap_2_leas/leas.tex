

\chapter{Guide path: LEAS}
\label{sec:LEAS}
\minitoc
\bigskip

\textcolor{red}{A revoir apres l'ecriture de la SOTA etc}
In this chapter, we present our method to Learn To Steer a locomotion contact planner \qq{LEAS}, the core module of this thesis. 
Our steering method answers the question: \textit{How to locally navigate complex and unknown terrains subject to validity and collision constraints?}
% Input / Output
LEAS takes as input a desired direction and a local heightmap of its surrounding terrain, in order to generate a robot root trajectory.
As many solutions exist to solve such a task, we provide an insight into our design choices and particularly why we use Reinforcement Learning.

In this section, we focus on the implementation of LEAS, which will be reused and adapted for three different contact planners through Chapters \ref{sec:CP-SB} and \ref{sec:CP-SL1M}.

This chapter is organized as follows: 
In Section \ref{subsec:leas-motivation} we describe the context and the challenges we want to solve. In Section \ref{subsec:leas-RL} we present the specification of the problem and our solution LEAS \cite{LEAS}. We describe it as an RL agent and explain our design choices for its observation, control, and reward to achieve the desired behaviour.
In section \ref{subsec:leas-implementation}, we present our implementation regarding the feasibility approximations, the random terrain generation, and the learning architecture.
%additional libraries implemented to optimize the computation time and to create random terrains where to train and evaluate our steering method.
In Section \ref{subsec:leas-results}, we evaluate how LEAS can navigate unknown terrains under reachability and collision avoidance constraints, and compare its results to some other model-based steering methods.
Finally, we discuss the advantages, limitations, and potential improvements of our local navigation method.

\section{Motivation\label{subsec:leas-motivation}}
\subsection{Context\label{subsubsec:context}}

\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{Figures/Chapter_LEAS/pipeline.png}
    \caption{Pipeline of the Loco3D project addressing the locomotion with a multi-stage approach.}
    \label{fig:pipeline}
\end{figure}


% Re-explain all the context with the division of the locomotion into three phases: Guide path planning - Contact planning - Whole body locomotion.
%In this work, we place ourself in a multi-stage framework \cite{loco3d} that divides the locomotion problem into three sub-tasks as shown on Figure \ref{fig:pipeline}: first \stn{c est quoi p1 ? definis. tu devrais dire que chaque p est un probleme} $P1$ generates a root trajectory, also referred as \qq{guide path}, then $P2$ generates a sequence of contacts on this guide, and finally $P3$ performs these contacts with a whole body controller on the robot.
As discussed in the previous chapter, most of the recent locomotion planners are structured in several hierarchical stages. Let us now present in detail how we structured our planner, based on the seminal work that drives the locomotion methodology in our team \cite{loco3d}.
The general organization is shown in Figure \ref{fig:pipeline}.

% All pipeline
A first stage generates a \textit{guide path} (P1), i.e. a trajectory in translation and rotation that we would like to follow (roughly ou exactly) with the main robot body, referred to as the \textit{root} (i.e. the basis of the torso for Talos). In the second stage, a contact planner (P2) computes the contacts along the guide path. In the last stage, a whole-body controller (P3) computes the control sent to the robot to perform these contacts.

% What we opt for: motion-before-contact
The decoupling of $P1$ and $P2$ has proven successful in \cite{Escande2008Guide, bouyarmane2009} and later on in \cite{loco3d, RB-PRM}, where each sub-task can be solved independently from each other, thus reducing the complexity of the problem and allowing us to experiment and compare different module implementations. 
\stn{il manque le papier de bretl, relis bien mon eta tde l art de these pour avoir lhistorique}
\textcolor{blue}{answer: Bretl and Hauser are contact-before-motion, they will be already cited in the SOTA.}

% Guide path definition
The first module $P1$ generates a guide path, defined in this thesis as a discrete sequence of configurations for the root of the robot in $SE(3)$ (i.e. position and orientation).
% Division strategy of path planning
This module can be further divided into two parts: a Steering Method (SM) and a path planning algorithm.
The SM locally navigates the terrain following a given direction, while path planning uses the SM to sequentially reach sub-goals (also called waypoints) up to a distant objective. 
On the resulting guide path, all configurations must respect two constraints as expressed in \cite{RB-PRM} and shown on figure \ref{fig:ROMs}.
\stn{ça va bien trop vite àa. C est un point important et differentiant. tu dois expliquer comment on fait du sampling based serieusement ici}
\textcolor{blue}{answer: Sampling-based for path planning you mean? Already done in the SOTA in the path planning section (but briefly). A REVOIR APRES L'ECRITURE DE LA SOTA.}

\begin{figure}
    \centering
    \includegraphics[width=0.3\textwidth]{Figures/Chapter_LEAS/ROMs.png}
    \caption{Validity constraints of the Talos Robot, (green) ranges of motion of each end effector and (red) the trunk of the robot. The configuration above is valid as the robot can potentially reach the ground and the trunk is not in collision with the terrain.}
    \label{fig:ROMs}
\end{figure}

The first constraint $\mathcal{R}$ is referred to as the \textit{reachability} and imposes that the robot, at a given root configuration, has a non-null contact reachable space (i.e must be able to touch the ground). 
This constraint is approximated as a reachability volume, which is a polytope representing the range of motion of one effector of the robot, shown on Figure \ref{fig:ROMs}.
As long as an intersection exists between the terrain and the reachability volume for a given configuration, we consider that this constraint is respected.
For simplicity's sake, we consider in this thesis only one reachability volume as the union of both legs polytopes.
The second constraint $\mathcal{C}$ is referred to as \textit{collision avoidance} and imposes that the robot must not be in contact with the terrain other than with its end effectors (foot and hands). This constraint is approximated as a polytope representing the robot trunk. If an intersection exists between this polytope and the terrain, the configuration is considered in collision.
In this manuscript, we will consider a configuration as valid if it respects both constraints $\mathcal{R}$ and $\mathcal{C}$, that approximate the feasibility of the problem by the next module (the contact planner).
% Steering methods at our disposal in Loco3D
We have two Steering Methods at our disposal in \cite{loco3d}:
\begin{itemize}
    \item RB-Lin, a linear interpolation between two configurations in $SE(3)$. In all our scenarios, we program RB-Lin to first interpolate on the orientation, to rotate the robot toward the goal with the an angular velocity $\omega_{max}$, then to interpolate on the position by moving the robot at $v_{desired}$ each timestep $T$ until reaching the goal;
    \item RB-Kino \cite{kinodynamic-sm} using a Double-Integrator Minimum-Time control \cite{DIMT}, and that is a kinodynamic SM connecting exactly two configurations in position, velocity, acceleration and orientation.
\end{itemize}
The main limitation of these SM is that they do not consider collisions and reachability with the terrain, therefore relying on path planning to validate these constraints along the trajectory.
In this work, we use a reachability-based probabilistic roadmap \cite{RB-PRM} as path planning algorithm with these model-based methods. 
% Explanation of steve algo (not my work so I won't go too much into it?)
Its concept is to generate candidate configurations by translating and rotating the robot root by a small increment until $\mathcal{C}$ and $\mathcal{R}$ can be met (full algorithm and details in \cite{thesis_steve}).
\stn{pareil ici c est complique, juste redonne l algo prm en fait et tu peux repartir de la apres. Il fait 10 lignes et tu peux le copier direct de ma these}
\textcolor{blue}{answer: Je l'ai vite fait mis dans la sota, et je ne l'utilise pas vraiment donc ce n'est peut etre pas utile de le dire. J'ai juste renvoyé vers ta these.}

\begin{figure}
    \captionsetup[subfigure]{justification=centering}
    %\centering
    \begin{subfigure}[t]{.49\linewidth}
    \includegraphics[width=\textwidth]{Figures/Chapter_LEAS/strategies_cp_guide_A.png}
    \caption{Find key contact configurations along the root trajectory\label{fig:strategies_cp_acyclic}}
    \end{subfigure}
    \begin{subfigure}[t]{.49\linewidth}
    \includegraphics[width=\textwidth]{Figures/Chapter_LEAS/strategies_cp_guide_B.png}
    \caption{Solve the surface selection problem for all steps\label{fig:strategies_cp_mip}}
    \end{subfigure}
    \caption{Contact planner strategies, (a) generates key configurations contact following the root trajectory in input, (b) discretizes the guide to find potential surfaces to step on.}
    \label{fig:strategies_cp}
\end{figure}

The second module $P2$ receives a guide path as input and populates it with a contact sequence. Such a contact planner can have different strategies on how to use the guide. In this thesis, we will explore two strategies for quasi-static contact planning with very distinct problem formulations. 
% Acyclic
The first (Figure \ref{fig:strategies_cp_acyclic}) uses the guide path directly as an exact root trajectory to follow and computes the contacts along it. The contact planner used in chapter \ref{sec:CP-SB} follows this strategy.
% MIP
The second uses the guide to get some candidate surfaces to step on, then solve a surface selection problem to have exactly one surface selected for each contact.
% Why this decomposition, benefits
The decomposition of the contact planning problem in P1 and P2 (motion-before-contact) breaks the algorithm complexity \cite{AcyclicCP,sl1m_v2} of contact-before-motion strategies, by constraining the search for contacts in the guide path vicinity.

Lastly, the module $P3$ performs the contact sequence on the robot with a whole body controller \cite{loco3d,tsid_prete_2016,Tonneau2018_2PAC,deepLoco}. We also implemented the environment to learn such whole-body controller via deep RL \cite{software_robot_RL} for future work.
In this thesis, we do not use the module $P3$ and focus on the contact planning problem ($P_1$ and $P_2$).


\subsection{Problem Statement\label{subsub:problematic_leas}}

% Motivation
In this context of division into two independent sub-tasks $P1$ and $P2$, the price to pay for the simplification of the contact planning problem is the absence of guarantees of feasibility between the different modules.
In such a sequential framework, the success of a module is a necessary condition for the success of the next in the pipeline but not sufficient.

That is why some approximations are made to increase the feasibility of the next module, such as the reachability condition in $P1$. The guide path planner assumes that if the ground is reachable at all times along the path, then it is possible to generate a contact sequence. 
This strategy has been proven effective on many scenarios \cite{AcyclicCP}, but can still fail as we will see in the next chapters, weakly approximating the capabilities of the contact planner plugged in. Each contact planner behaves differently as seen on Figure \ref{fig:strategies_cp} and we do not know what makes an easy guide path for it. As a consequence, it is difficult to define new additional constraints or heuristics in our navigation module ($P1$) to better approximate the contact planning feasibility ($P2$).

% Given the previous context, what do we want ?
We want to fix this issue with a high-level approach, on the very first module of the pipeline $P1$.
The problem we are trying to solve can be formulated as the question: \textit{What is a good guide path for a given contact planner ?} 
In other words, can we build a guide path planner that will better approximate the feasibility space of the contact planner. Such a planner could improve the success rate of $P2$ and sequentially, the success of the whole pipeline.

% Why do we focus on the steering method
As explained previously, $P1$ can be decomposed into two components: a path planning algorithm generating some waypoints to reach, and the SM that sequentially connects these waypoints.
In this thesis, we focus on the steering method that locally generates a guide path between two waypoints.
We want to build a SM able to locally navigate while observing the surrounding terrain to avoid obstacles and validate the reachability condition. 

% Why not working also on path planning ?
% If we have a good SM, then it makes the control a lot more easier for path planning algorithm.
% Path planning algorithm uses the SM to define control points (waypoints) to reach an objective. The SM returns if it succeeded or not to reach this waypoint. A too simple steering method unable to grasp the complexity of the terrain will have a lot of troubles to follow far away waypoints and will makes the search very difficult for the path planning algorithm, requiring more 
The reason why we do not further investigate path planning in this work is that in most simple scenarios, such as walking on flat ground or climbing some stairs, a SM should be enough to generate a valid trajectory. Doing so removes the need for a path planning algorithm (RRT,PRM) expensive to compute.
%In the future, the implementation of more efficient path planning algorithm as \cite{RL_RRT} will be implemented.

% What's our solution and why do we use RL for that ?
We build such a terrain-aware steering method with deep reinforcement learning.
The choice of using RL is motivated by its capabilities to learn a representation of its environment and to act in it while maximizing the reward and keeping a valid state.
A SM learned by reinforcement could learn how to navigate locally with a vision of its surrounding terrain, while respecting the validity constraints and increasing the success rate of the contact planner.

In this chapter, we first train a pure steering method and provide the results. 
In the next chapters, we plug a contact planner to validate the trajectories during the learning, then we observe what behaviour changes on the SM to increase its success rate.


\subsection{Reinforcement Learning: Overview}

Reinforcement Learning can be defined as an agent interacting with an environment according to a policy $\pi$. 
The interaction sequence can be modeled as a Markov Decision Process (MDP) that is a tuple ($S$,$A$,$P$,$R$,$\mu$,$\gamma$), where:
\begin{itemize}
    \item $S$ is the state space of the environment.
    \item $A$ the set of discrete or continuous actions available.
    \item $P: S \times A \rightarrow \Delta S$ is the transition function of the MDP dynamics.
    \item $R: S \times A \rightarrow \mathbb{R}$ is the reward function defining the desired agent behavior.
    \item $\mu$ is the initial state distribution.
    \item $\gamma \in [0,1]$ a discount factor.
\end{itemize}

The agent starts an episode in an initial state $s_0$ sampled according to $\mu$.
Depending on the agent state $s_t \in S$, the objective of the policy $\pi$ is to make the agent perform an action $a=\pi(s)$ in its environment, while maximizing its future cumulative rewards $J(\pi) = \mathbb{E}[\sum_{k=t}^{\infty} \gamma^k r_{t+k} | \pi]$.
We denote the optimal policy $\pi^* = \underset{\pi}{\mbox{arg max}} \; J(\pi)$.

In this work, we will use a partially observable MDP as our steering method is only local and can not observe to the entire state space.


\section{LEAS: a RL Steering Method\label{subsec:leas-RL}}
% What behaviour do we want for the SM ?
Given initial and goal configurations, we learn by reinforcement a steering method generating a guide path in a given direction while respecting both collision and reachability constraints. 
To this end, we designed LEAS \cite{LEAS}, a RL agent that learns by trial and error how to perform this task and to generalize to unknown environments. An overview of our method is presented in figure \ref{fig:LEAS}. The main idea behind LEAS is that for each action taken in the environment, the configuration in $SE(3)$ of the robot will be modified and stored in a list: the guide path.

\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{Figures/Chapter_LEAS/leas_overview.png}
    \caption{Overview of LEAS: The user or a path planning algorithm inputs a target direction to our steering method. LEAS is an RL agent that moves the robot in the environment and sequentially save all its states. Once the episode is over, this state sequence is the guide path given to the contact planner $P2$. Finally, P2 returns an evaluation of the contact plan generated that makes LEAS learn how to generate better guide path for it.}
    \label{fig:LEAS}
\end{figure}

In this section, we will take a look at the result of LEAS without plugging it to a contact planner.
Explanations on how LEAS can improve the success rate of a given contact planner will be let for chapters \ref{sec:CP-SB} and \ref{sec:CP-SL1M}.

\subsection{Specifications \label{subsubsec:specifications}}
% What behaviour do we want LEAS to have.
% Goes in depth point by point on every specification of the first sentence, translating it into RL actions/states etc.
We need to specify the task we want to achieve with LEAS, with what behaviour, and how to achieve it with Reinforcement Learning.

We choose to input a goal direction in LEAS instead of a goal configuration as done in other works \cite{kinodynamic-sm,DIMT,RL_RRT}.
We defined a steering method as a planner generating a trajectory ending closer to the goal than initially, and that is exactly what LEAS does by moving toward the goal direction.
This input also allows for more flexible controls by the user that can set a fixed goal (reachable or not) in the environment or steer in a direction with a joypad \cite{AcyclicCP}.

Following this design, we desire LEAS to have the following list of behaviours:
\begin{enumerate}[label=(\Alph*)]
  \item Move in the goal direction at a fixed desired velocity.
  \item Avoid collisions and respect the reachability constraint at each state. 
  \item Stop if it can not go further (i.e keep a valid state).
  \item Navigate locally without the help of path planning, meaning that LEAS should only act based on a very short vision range.
  \item Orientate its root in the desired goal direction. This design choice can be limiting in cluttered environments where side walking is required, but was necessary in our experiments to obtain a straight walk (sidewalking is the most robust strategy for walking and was predominant without this specification).
\label{list:leas:specifications}
\end{enumerate}

The design of an RL agent for our navigation task requires four main components: 
the agent \textit{state} that represents what the agent sees from the environment, the \textit{actions} the agent performs according to its actual state, the \textit{reward} that evaluates how well the agent acts to achieve the desired behaviour, and finally the \textit{done} condition that checks if an episode is over.

In our work, the \textit{done} condition is straightforward as it verifies the collision and reachability constraints. If the actual configuration of the robot cannot reach the ground or is in collision with the environment, the episode is over. 
As a result in order to maximize the future cumulative rewards, the agent learns by reinforcement how to avoid such cases and to fulfill the behaviour (B). 
We can add two optional conditions depending on the scenario played to stop the episode: when reaching a maximum number of steps, or when being close enough to an objective.





\subsection{States\label{subsubsec:states}}

\begin{figure}
    \centering
    \includegraphics[width=0.42\textwidth]{Figures/Chapter_LEAS/leas_states.png}
    \caption{Robot states: (Blue arrow) velocity of the robot, (White arrow) orientation, (Red arrow) desired velocity and orientation, (Dots) heightmap relative to the orientation.}
    \label{fig:LEAS_states}
\end{figure}

The robot configuration can be seen in figure \ref{fig:LEAS_states} from which we can get the observable states driving the actions of LEAS. The desired velocity and orientation are represented by the same vector and expressed implicitly in our states.

The observable state is a set: 
\begin{equation}
S=\{v_{o}, o_{target}, h_{o}\}
\end{equation}
with $v_{0}$ the velocity of the robot relative to its orientation, $o_{target}$ the angle between its actual and desired orientations, and $h_{o}$ a local heightmap of the terrain relative to its $z$ root position.

\begin{figure}[h]
    \captionsetup[subfigure]{justification=centering}
    %\centering
    \begin{subfigure}[t]{.49\linewidth}
    \includegraphics[width=\textwidth]{Figures/Chapter_LEAS/hm_small_vision.png}
    \caption{\label{fig:vision_hm_size_short}}
    \end{subfigure}
    \begin{subfigure}[t]{.49\linewidth}
    \includegraphics[width=\textwidth]{Figures/Chapter_LEAS/hm_big_vision.png}
    \caption{\label{fig:vision_hm_size_big}}
    \end{subfigure}
    \caption{Vision field of LEAS for different local heightmap sizes.}
    \label{fig:vision_hm_size}
\end{figure}

% - Heightmap for the agent to understand locally its surrounding. 
% Explain the dimensions, the number of dots, the space between them. 
The dimensions of the heightmap are 7 values in front of the robot, 3 in the back, and 7 values on each side with a discretization step (rounded) of $15$ cm, $17$ cm, and $9$ cm respectively. This roughly corresponds to a short vision range of $110$ cm in the front, $50$ cm in the back, and $60$ cm on each side of the robot (figure \ref{fig:vision_hm_size_short}).

% Why not bigger ?
The observable heightmap is small on purpose as we desire a steering method with the behaviour (D) (\ref{subsubsec:specifications}), to navigate locally. Increasing its visual field leads to an over-fitting on our training terrain and the emergence of path planning behaviour.
Indeed, if we give too much information about the surrounding terrain (figure \ref{fig:vision_hm_size_big}), one could memorize its topography, guess its global position on it and plan its path through it \cite{rl_navigation_video_game_2020}.
Empirically, we found that $110$ cm in front and $60$ cm on each side of the robot was enough for LEAS to detect the obstacles and react in consequence.
% Is this discretization enough ?
The heightmap resolution of LEAS is $15$ cm, compared to previous works as \cite{RLOC, deepGait, deepLoco} that have a resolution of 2cm, 4cm, and 34cm respectively, and is sufficient to navigate the training ground and to generalize to unknown terrains. 
Increasing the resolution did not improve the learning or navigation skill of LEAS for our test scenarios, but may be required in the future for more complex scenes with elements of smaller surfaces (handrails detection for example).

\begin{figure}
    \captionsetup[subfigure]{justification=centering}
    %\centering
    \begin{subfigure}[t]{.49\linewidth}
    \includegraphics[width=\textwidth]{Figures/Chapter_LEAS/hm_bounded_high.png}
    \caption{Wide heightmap bounds}
    \end{subfigure}
    \begin{subfigure}[t]{.49\linewidth}
    \includegraphics[width=\textwidth]{Figures/Chapter_LEAS/hm_bounded_low.png}
    \caption{Small heightmap bounds}
    \end{subfigure}
    \caption{Observable heightmap with $z_{min}$ and  $z_{max}$ relative to $z_{root}$ and (blue dot) the bounded heightmap values.}
    \label{fig:heightmap_bounds}
\end{figure}

% What are the bounds for this relative heightmap ? Why not bigger bounds ?
All $z$ values in the local heightmap are bounded by $[z_{min}, z_{max}]$. The tuning of these two bounds depends on the topography of the terrain where LEAS will navigate. 
A wider interval means that the RL agents will better dissociate higher height variations (stairs and obstacles) in the terrain at the cost of a lower resolution for small variations (rubbles and small slope).
This is illustrated in figure \ref{fig:heightmap_bounds} where wider bounds (a) are a better representation for obstacles detection, and smaller bounds (b) make the agent more sensitive to small variations of height but may mistake the distant obstacle as a stair. 
Empirically, we find a right balance of $z_{max}= -0.2$ and $z_{min}= -1.4$ meters from the robot root, to be a good balance for LEAS to visualize both small and high variations.

\subsection{Actions\label{subsubsec:actions}}
At each step, the RL agent takes action in the environment based on of the observed state. 

Our policy returns a set of actions: 
\begin{equation}
A = \{ a_x, a_y, a_z, \omega \}
\end{equation}
with $a_x, a_y, a_z$ the accelerations of the robot on each axis, and $\omega$ the angular velocity of the robot on the yaw axis. At each step $i$, the robot position $q_{pos}$, velocity $q_{vel}$ and orientation $q_{ori}$ are modified in the following order:
\begin{equation}
\begin{cases}
(a) \; q_{pos}^i = q_{pos}^{i-1} + q_{vel}^{i-1} * \Delta T \\

(b) \; q_{vel}^i = q_{vel}^{i-1} + [a_x, a_y, a_z] * \Delta T \\

(c) \; q_{ori}^i = q_{ori}^{i-1} + \omega * \Delta T 
\end{cases}
\end{equation}
with $\Delta T$, a user-defined timestep, $q_{pos}$, $q_{vel}$ and $q_{ori}$ the global position, velocity and orientation of the robot respectively.

The velocity $q_{vel}$ and $\Delta T$ impacts the number of configurations along the guide path. For a constant velocity along the guide of $0.10$ m/s and $\Delta T=0.2$ seconds, the discretization step between each configuration is $2$ cm. 
We further bound $q_{vel}$ in order to keep its norm $|q_{vel}| \leq v_{max}$ with $v_{max}=0.2$ m/s. %All parameters can be seen on table \ref{tab:param}.

The choice of $\Delta T$ depends on the contact planner used. 
Empirically, we know that the contact planner \cite{AcyclicCP} has a higher success rate for the discretization step inferior to $\approx 15$ cm.
For LEAS, we choose to set $\Delta T=0.2$ seconds which for a $v_{max}=0.2$ m/s results in a maximum discretization step of $4$ cm, and we further discretize the guide if required depending the contact planner.
These values have been selected empirically to achieve the specified behaviours while minimizing the number of steps required to generate a guide path potentially successful with our contact planners (discussed in the next chapters).

% Why this order for the control in acceleration ? It induces a delay in the action
With the order in which the robot states are updated, we induce a delay in the action of the agent. 
The position of the robot is updated with the previous velocity (a), then the acceleration action updates the velocity (b). As a result, the agent sees the impacts of its actions directly on the velocity, but not on the robot position and so its observable local heightmap.
In RL, problems where the actions are not instantly applied on the states or captured by the observations have been termed as Delayed MDPs \cite{RL_delayed}. 
Our control does not fall directly into this category as the agent can see its action directly on the velocity. However, one can ask if inverting the order of (a) and (b) and removing this delay on the position could improve the learning and results of our agent. 
In practice with recent RL algorithms, fixed delays of one or two steps do not matter \cite{RL_delayed_explanation} as long as the agent has enough steps left to react to an event. We verify it on figure \ref{fig:control_LEAS_learning_curves} that shows no difference in the learning of both controls with our parameters.

% Controls: why control in acceleration ?
We choose for LEAS a control in acceleration to get smoother trajectories, where changes in the velocity along the trajectory are limited by maximum acceleration.
In our experiments, we find that having a low acceleration with a small $\Delta T$ helps the exploration and leads to a more stable training overall, where the actions have a lower impact on the system compared to a control in velocity.
For LEAS, we keep a maximum acceleration high enough to be able to react to new observations as the detection of an obstacle, but low enough to help the training and generate smooth trajectories.

% Why not just giving the configs for each step ?
Another option is to control in velocity, with an action corresponding to the next velocity $q_{vel}$ and $\Delta T$ high enough to have one configuration per footstep (almost equivalent to control in position).
% Removed: In our experiments, this implementation does not work well with our sample-based contact planner of chapter \ref{sec:CP-SB} that prefers guide path with a small discretization step, but is pertinent for those of chapter \ref{sec:CP-MIP} and \ref{sec:CP-SL1M}. 
While it is pertinent to generate one root configuration per footstep, it is also more difficult to train as it requires a few numbers but critical actions to generate a guide path. 
In our experiment, training LEAS with such control in position is inefficient as it drastically lowers the probability to end in a feasible state, thus requiring additional strategies to guide the exploration.
As a consequence, we choose to keep control in acceleration which leads to more stable learning for LEAS with and without a contact planner.

\subsection{Rewards}
% What do we want our robot to do ?
As written in the specifications (section \ref{subsubsec:specifications}), the behaviours we have yet to obtain are: (A) move the robot in the goal direction at the desired velocity, (C) stop if it can not go further, (E) orientate its root in the goal direction. 
We design three rewards to get these behaviours:
\begin{enumerate}
  \item[(E)] $R_{ori} = -( 1 -  \overrightarrow{q_{ori}} \cdot \overrightarrow{u}_{target} )$ \\
  with $\overrightarrow{q_{ori}}$ the unit vector representing the root orientation and $\overrightarrow{u}_{target}$ the goal direction.
  \item[(A)] $R_{dir} = -( ||\overrightarrow{q_{vel}} - \overrightarrow{v*} ||/(2v_{max}) )^2$ \\
  with $\overrightarrow{q_{vel}}$ the velocity vector, $\overrightarrow{v*}=\overrightarrow{u}_{target} \times v_{desired}$ the desired velocity vector and $v_{max}$ the max velocity norm.
  \item[(C)] $R_{alive} = $ 
    $
    \begin{cases}
      1, & \text{if next state is valid} \\
      0, & \text{otherwise}
    \end{cases}
    $
\end{enumerate}
The reward $R_{ori}$ penalizes the agent for not being oriented toward the goal (E) and $R_{dir}$ penalizes it for not moving in this direction as the desired velocity (A). 
As both rewards are negative, using only these two leads the agent to terminate the episode as soon as possible to avoid the accumulation of negative rewards. 
Therefore we introduce another positive constant reward $R_{alive}$ that the agent get at each step to encourage him to keep a valid configuration. 
As a result, the agent learns that in order to maximize the future rewards, when it is not possible to move further in the desired direction, it is better to stay idle rather than to terminate the episode, thus fulfilling the behaviour (C).

To smooth the trajectory, we add two rewards to punish the agent when taking large actions:
\begin{enumerate}
    \item $R_{\omega} = - | \omega / \omega_{max} |^2 $ \\
    with $\omega$ the action on orientation and $\omega_{max}$ the max angular velocity.
    \item $R_{acc} = - (|| [a_{x},a_{y},a_{z}] || / a_{max})^2 $ \\
    with $[a_{x},a_{y},a_{z}]$ the action taken on acceleration and $a_{max}$ the max acceleration.
\end{enumerate}

The resulting reward is :\\
$R = R_{dir} w_{dir} + R_{ori} w_{ori} + R_{\omega} w_{\omega} + R_{acc} w_{acc} + R_{alive} w_{alive}$\\
with $w_{dir}$, $w_{ori}$, $w_{\omega}$, $w_{acc}$ and $w_{alive}$ some user-specified weights.

Another possible reward design is to let $R_{dir}$ be positive as done for the High-level controller of DeepLoco \cite{deepLoco}. 
For example, we can set $R_{dir}=-(||\overrightarrow{v} - \overrightarrow{v*}||/(2 v_{max}))^2 + 1$ and remove $R_{alive}$, which in that case is strictly equivalent to our formulation above with $R_{alive}=1$ and $w_{alive}=1$.
That is why for the sake of clarity, we separate these two rewards. 
An advantage of this separation is that we can increase $w_{alive}$, relative to the other reward weights, to force the agent to act carefully and to stay away from dangerous situations as staying too close to an obstacle. 
In our experiments, keeping $w_{alive}=1$ is sufficient to get the desired behaviour (C), but setting it too high as $w_{alive}=2$ makes the agent act too safely and stay idle instead of crossing difficult obstacles.

\section{Implementation details\label{subsec:leas-implementation}}

% Problem with using HPP like that:
% - HPP is made for scenarios on small scenes with a low number of surfaces
% - HPP can run several clients, but main use: perform scenario, close it, then restart it. No easy reset.
% - Very slow to load big scenes
% - No tool to get the heightmap
% - The scenes are fixed and we can not change it => One instance = one scene
% - Functions of HPP are done in C++ (fast), but may be slower if intensively called again and again as configuration validity => Need a better way to check validity (collision+reachability) => Approximations using HM.

We train LEAS using HPP software \cite{HPP_software} to load the terrain and the Talos robot model \cite{talos_robot}. 
To accelerate the training time and have an agent with good generalization capabilities, we want to have several agents in parallel during the training to generate the trajectories on several or one big scene. 
% Limit of HPP
While it is possible to have several clients of HPP on the same machine, this software was mainly implemented for short scenarios on small terrains, but not for intensive usage as we do in RL or to load scenes with thousands of surfaces. 

To that end, we implemented a python library to extract a heightmap from the meshes of the scene, and we present two main tools to be implemented in HPP later on: an approximation of the collision and reachability constraints from the heightmap, and a random terrain generator. 
Finally, we present an asynchronous version of the RL algorithm PPO to perform the contact planning sequence externally with a master-worker architecture.

\subsection{Validity approximation\label{subsub:validity_approximation}}
% Explain how it is done in HPP
% Why the need to approximate it => Remove dependencies with HPP that was made for smaller scenes + Faster and simplified, enough for LeaS.
% I approximated the ROM => bounds for each point on the rel HM. Easier to tune it, without needing to cut the ROM files.
% Reachability: If all points of ROM on rel HM bellow bounds => unreachable.
% Stricter reachability: points of ROM (middle) must be above bounds.
% Collision: If one above on mid point of rel HM above bounds => collision.
The validity of a configuration is subject to two constraints $\mathcal{R}$ and $\mathcal{C}$ collisions and reachability.
Efficient functions are available in HPP to check these constraints by verifying if an intersection exists between either the ROMs of the robot legs or the volume representing its trunk and surfaces of the terrain. 
However in our first version of LEAS \cite{LEAS}, the computation time of these validation functions was significant (more than 15\% of the computation), and as a consequence we opted for a faster and more flexible implementation of these constraints.

\begin{figure}
    \captionsetup[subfigure]{justification=centering}
    \centering
    \begin{subfigure}[t]{.48\linewidth}
    \includegraphics[width=\textwidth]{Figures/Chapter_LEAS/approx0.png}
    \caption{\label{fig:approximation_validity_0}}
    \end{subfigure}
    \begin{subfigure}[t]{.48\linewidth}
    \includegraphics[width=\textwidth]{Figures/Chapter_LEAS/approx1.png}
    \caption{\label{fig:approximation_validity_1}}
    \end{subfigure}
    \begin{subfigure}[t]{.48\linewidth}
    \includegraphics[width=\textwidth]{Figures/Chapter_LEAS/approx2.png}
    \caption{\label{fig:approximation_validity_2}}
    \end{subfigure}
    \begin{subfigure}[t]{.48\linewidth}
    \includegraphics[width=\textwidth]{Figures/Chapter_LEAS/approx3.png}
    \caption{\label{fig:approximation_validity_3}}
    \end{subfigure}
    \caption{Approximated validity conditions: (Green) robot range of motion, (blue) heightmap. Configuration (a) is valid, (b) is valid with $\tilde{\mathcal{R}}$ but invalid with $\tilde{\mathcal{R}^*}$ where the highlighted blue dots are not reachable, (c) and (d) are in collision.}
    \label{fig:approximation_validity}
\end{figure}

That is why we approximate the collision and reachability constraints by $\tilde{\mathcal{R}} \subset \mathcal{R}$ and $\tilde{\mathcal{C}} \subset \mathcal{C}$ respectively, using the local heightmap relative $H$ to the root of the robot.

Offline, we assign to each point $p_i$ of $H$, some reachability $\tilde{\mathcal{R}_i}$ and collision $\tilde{\mathcal{C}_i}$ constraints on its height $z_i$. 
A point $p_i$ respects $\tilde{\mathcal{R}_i}$ if it is inside the robot range of motion, and respects $\tilde{\mathcal{C}_i}$ if it is not inside or above the robot trunk volume as depicted on figure \ref{fig:approximation_validity}. 
The approximated reachability condition is valid if:
\begin{equation}
    H \in \tilde{\mathcal{R}} \Rightarrow \exists p_i \in H, p_i \in \tilde{\mathcal{R}_i}
\end{equation}
The approximated collision condition is valid if:
\begin{equation}
    H \in \tilde{\mathcal{C}} \Rightarrow \forall p_i \in H, p_i \in \tilde{\mathcal{C}_i}
\end{equation}
Such approximation requires the root of the robot to only rotates on the yaw axis when navigating, that is a common assumption for biped locomotion \cite{CP_MIP_DEITS}.

One drawback of using a single layer heightmap for this validity condition is its limitation to environments without an upper floor or ceiling.
% Collision detection
The collision detection from a heightmap is verified by the \textit{not above} robot trunk volume condition. 
The trivial case is that if a point $p_i$ lies inside this volume, there is a collision (figure \ref{fig:approximation_validity_2}). However, we do not know if there is a collision if a $p_i$ lies directly above the robot. 
Two cases are possible: first, the robot is under an object (ceiling) and not in collision; second, the robot is inside the object and so is in collision (figure \ref{fig:approximation_validity_3}).
From the heightmap, we cannot distinguish these cases and thus we choose to consider both as a collision.
% Conclusion of this drawback
AS all our testing terrains do not contain an upper floor and we do not perform any locomotion task as passing under a gate, such approximation is sufficient. 
In the future, another solution will be required to navigate such scenes such as using two layers heightmap or voxels to better visualize the geometry of the terrain or using alternative validity conditions.

% No longer true with the formulation above
% is that it considers upper floors and ceilings as obstacles. However as we do not perform scenarios as passing under a gate or crouching to move under a low ceiling, such approximation is acceptable. A solution to this limitation in a future work could be to use two heightmaps, one for the closest ground floor and a second one for the upper ceiling.  \stn{ou simplement pas mettre le pied plus haut que le torse ?}

% Depending on the discretization of the heightmap, it can happen that the reachability constraint is not fulfilled even if graphically, we can see that the ground is reachable. 
% If a config is invalid with the approx, recheck :
% (1st option) Use isConfigValid of HPP.
% (2nd option) Create a convex hull from the approximated reachability bounds, and check if there is an intersection with any of the potential surfaces
Another limitation of these approximations comes from the heightmap resolution can fail to approximate the constraints $\mathcal{R}$ and $\mathcal{C}$.
% What happens
% Reachability
On the reachability, a low heightmap resolution can fail to visualize a scene composed of small and spaced surfaces and can consider wrongly that $H \notin \tilde{\mathcal{R}}$ (i.e. the terrain under the robot is not reachable) when in reality $H \in \mathcal{R}$.
To remove such cases, we reconfirm the invalidity of the configuration with the full constraint $\mathcal{R}$ before terminating the episode.
% Collision
As for the collisions, the opposite happens where a collision is not detected with the approximation, $H \in \tilde{\mathcal{C}}$ when in reality $H \notin \mathcal{C}$ and so only the full validation $\mathcal{C}$ can detect this case.
In practice, LEAS learns to keep a safe margin between the obstacles and the robot trunk to avoid collisions and so, implicitly avoid such collision detection cases.
% Trade-off
As a result, we opt for a trade-off between the fast to compute approximated validity constraints $\tilde{\mathcal{R}}$ and $\tilde{\mathcal{C}}$ checked at every step of the guide path generation, where the full reachability condition $\tilde{\mathcal{R}}$ is only performed to confirm the invalidity of a root configuration.

\begin{figure}
    \centering
    \includegraphics[width=0.42\textwidth,height=4.5cm]{Figures/Chapter_LEAS/hole_scenario_above_void_p1.png}
    \includegraphics[width=0.42\textwidth,height=4.5cm]{Figures/Chapter_LEAS/hole_scenario_above_void.png}
    \caption{Valid configuration with $\tilde{\mathcal{C}}$ and $\tilde{\mathcal{R}}$ leading to a blocking configuration above the hole.}
    \label{fig:hole_scenario_above_void}
\end{figure}

Finally, we add one additional constraint to further improve the quality of the guide for the biped walking task. 
One default of the original validity function is that configurations above the void but touching the edge of the terrain with the range of motion are considered valid (Figures \ref{fig:approximation_validity_1} and \ref{fig:hole_scenario_above_void}), leading to infeasible trajectories by our contact planners.
To avoid such configurations, we add another validity condition on the heightmap $H^*$ located directly under the root of the robot ($H^*$ illustrated by the white dots of the heightmap on Figure \ref{fig:LEAS_states}) to be reachable at all time. 
We name such reachability constraint with the stricter condition $\tilde{\mathcal{R}^*}$ where:
\begin{equation}
    H \in \tilde{\mathcal{R}^*} \Rightarrow \exists p_i \in H^*, p_i \in \tilde{\mathcal{R}_i}
\end{equation}

With this strategy, configurations where the robot stands above the void are considered invalid. 
This can be a limitation on terrains with very spaced surfaces, but in all our scenarios this condition offers good results in terms of guide path quality.
This leads to the generation of a guide path with fewer states above the void and so a faster computation time with our contact planners.
This solution was not yet implemented in our first version of LEAS \cite{LEAS} where we learned by reinforcement such additional constraints using the contact planner as a guide path validator.

In the context of RL performing millions of steps, such approximation with $\tilde{\mathcal{R}^*}$ and $\tilde{\mathcal{C}}$ significantly improves the training time. 
While the gains in computation can not be fairly compared due to their difference in programming language and optimization, our implementation resulted in an average of 20 times faster computation time compared to the full constraints $\mathcal{R}$ and $\mathcal{C}$, thus allowing us to train and test more models.


\subsection{Terrain generator\label{subsub:terrain_generator}}
% Originally made by daeun, and adapted by me to produce arenas of size NxM
% That is somehow a huge part of my work that was needed for LeaS.
% Give a link to the code as well, but say that some fix need to be made.
Our goal is to train a single policy to navigate all our key scenarios. 
An RL agent generalizes better if it learns on diverse terrains. 
To that end, we extended the library used in \cite{sl1m_v2} to generate random terrains composed of rubbles, stairs, bridges, and obstacles. The code is available on GitHub \cite{random_scene_gen}.

\begin{figure}
    \centering
    \includegraphics[width=0.42\textwidth]{Figures/Chapter_LEAS/random_scene_tiles_example.png}
    \caption{Example of a random scene generated, containing different types of tiles connected with each other: flat ground with and without obstacles, stairs, rubbles, and bridges. \textcolor{red}{\textbf{TO DO}: Show all types of terrain one by one.}}
    \label{fig:random_scene_gene_tiles_example}
\end{figure}

This library generates a terrain that we can divide into tiles. 
Each tiles belong to two main categories: \textit{Initial} or \textit{Transition} (figure \ref{fig:random_scene_gene_tiles_example}). 
The first category \textit{Initial} contains two types of tiles: flat ground with or without obstacles. 
The second category \textit{Transition} connects two \textit{Initial} tiles and contains three types of tiles: stairs, rubbles, or bridges.

We represent the terrain as a grid of fixed-size tiles and starting from an \textit{Initial} tile, we build a tree filling pseudo-randomly its empty neighbors. Each \textit{Transition} is unique as the characteristics of each surface composing it are random and depend on the height of the tiles it connects.

At the end of the terrain generation, we can identify all the links created. 
A link is defined as a sequence of \textit{Initial} - \textit{Transition} - \textit{Initial} tiles in a line, and that we can further divide into several categories in function of their difficulty. 
Finally, we extract the possible starting position for the robot for each link, corresponding to areas on \textit{Initial} tiles. 

Terrains generated by this library can contain enough elements, depending on the dimension of the grid and the random seed, for the agent to develop its capability to generalize to most of our test scenes, that contain the same kind of transitions.

% Some limitations of using this library to generate terrains for the training
% (1) All tiles have the same size, it can be a problem depending on the observable heightmap of the agent, for example it will never learn how to climb very long stairs if the hm is too big.
% (2) It is limited to the kind of transitions that are implemented. Very different obstacles as walking on plateforms or weird scenarios may not work with an agent trained on our terrain. Even though our arena is big and contains enough element for the agent to figure out most of our test scenarios.
% (3) These scenes are still fixed, no dynamic elements. Ok for our test scenarios.
% (4) The quality of the agent depends on the scene it's been trained on. We have to carefully see if the generated scene contain enough of each tile types.
Some limitations come with the use of this library for our training.
The first is that all the tiles of one scene have the same size, which we fixed to 2x2 meters for all our training scenes. 
This can be a problem and have to be tuned according to the size of the observable heightmap by our agent, as a too large visual field could lead the agent to specialize in navigating on the scene with this tile size where the agents will always have to cross stairs or bridges of 2 meters length. 
In our experiments, with a visual field of $110$ cm in the front, as explained in \ref{subsubsec:states}, our trained agent is able to learn from 2x2 meters tiles how to generalize to our other scenarios.
The second limitation is that even though the \textit{Transition} tiles are all unique and randomized, they still fall into the same categories (i.e. stairs, rubbles, bridge, obstacle) and we could question the ability of the agent to navigate on other very different terrains. Empirically this was not a problem to generalize to all our test scenarios that contain almost the same type of elements, but a higher diversity of elements may be required in the future.
Finally, the performance of the agent also depends on the size of the terrain it has been trained on. We had to generate several random arenas before finding one having enough of each element and link. 
In our experiments, terrains of dimension 5x30 was the maximum we could have in HPP as the loading of the terrain and computation time exponentially increases with the number of surfaces in the scene.

% Options tried
% (1) All parallel agents during the training evolves on different terrain, possibility to change this terrain, going from an easy one first to a more difficult one later on => Curriculum learning. Then we run one or several contact planners for each type of terrain, that will compute the CP only for these terrains.
% (2) Have just one huge scene that has enough diversity of obstacles / transitions to learn how to generalize.
% A combination of (1) and (2) is also possible.
 Several options can be added to learn from the random scenes generated.
 We can have several parallel agents learning on different scenes or modifying the scene during the training.
 We can also use the difficulties of each link in the scene to perform curriculum learning \ref{curriculum_learning_survey}, starting with the easiest task as walking on flat ground to the most difficult links, climbing stairs.
 In a first test, we ran several agents during the training on different terrains following a curriculum learning \cite{curriculum_learning_survey}, starting with a flat ground, then stairs, and finally, a 5x30 terrain randomly generated (section \ref{subsub:terrain_generator}). However, it did not further improve the learning time, as our agent is able to learn quickly without these methods from only one 5x30 scene for all our agents.
 
 
\subsection{Master-Workers architecture: asynchronous contact planners\label{subsub:leas:master_worker}}
We use the RL algorithm Proximal Policy Optimization (PPO) to learn LEAS.

% Why did I pick PPO (online) and not an offline RL algo
Several state-of-the-art RL algorithms are available and can be separated into different groups, each presenting some pros and cons, and that can lead to various results depending on the hyperparameters used and the task to perform \cite{RL_that_matters, RL_that_matters_OnPolicy}. As a consequence, it can be difficult to judge which algorithm to use for a given task other than empirically. However, some comparisons are available to guide our choice \cite{compare_PPO_TD3_SAC_DDPG}. 

% What do we try ?
We reduce our choices to two State-of-the-Art RL algorithms that are PPO \cite{PPO} that is on-policy and Twin Delayed DDPG (TD3) \cite{TD3} off-policy, both widely used in the literature and implemented in Stable Baseline \cite{stable-baselines} that we can adapt for our task.  
\stn{pourquoi? c est quand même le plus important en tant que contribution ici et tu zappes ça en deux secondes. il faut mieux justifier le choix}
\textcolor{blue}{Answer: Nobody says why most of the time. I do a justification offline vs online after, I think it's enough.}

% What are the main pros and cons
Off-policy algorithms (TD3, SAC \cite{SAC} ...) are known to be more sample efficient but can lack stability compare to on-policy RL algorithms.
% What do we pick in the end ?
% Note: TD3 is hard to tune, it did not converge after 3M steps, when PPO already converged.}
That is why most of the recent works in RL often use PPO \ref{survey_rl_animation_pettre_2022}, that can be easier to tune and more stable during the learning, even for tasks that require data-efficiency as \cite{openai2019dota}.
%The learning of LEAS without a contact planner, which is a pure navigation task can be learned efficiently with PPO as in \cite{compare_PPO_TD3_SAC_DDPG} for a boat navigation task.
In our first version of LEAS \cite{LEAS}, the training with PPO was slow due to its poor sample efficiency (order of hours without contact planner and tens of hours with our sample-based contact planner). %At that time, using off-policy RL algorithms was a more pertinent choice.
However, this issue was fixed with the optimizations and approximations discussed in the previous sections making both off and on-policy pertinent choices.
We tested both TD3 and PPO for our navigation task and we observed with TD3 a slower and less stable convergence than PPO, with the default hyperparameters as defined in Stable Baseline or with manual tuning.
As a result, we decided to use PPO to learn LEAS.

% What is the main problem of LEAS design, how to modify PPO to make it works ?
% Why only asynchronous for the CP ? Why master-worker ? Are there some limitations / alternatives ? => Limitation of our software + exchange of datas between processes + the CP is the only long thing to compute and just need the guide path to be done.
Given the design of LEAS (figure \ref{fig:LEAS}), we have two components to consider during the training: first, our steering method is a policy taking actions in the environment and saving the sequence of states as a guide path, then the contact planner that will generate the contact sequence and give a feedback of its result to LEAS. 
%We do not define the feedback in this section that depends on the contact planner used and the type of reward we want to implement.
In this design, the main problem is the contact planning validation of the guide path at the end of each episode. 
If we simply perform this contact planning inside each agent, as all parallel agents are steps synchronous (or trajectory synchronous), if one agent computes a contact sequence, all the other agents have to wait for it to finish, and thus the computation time increases with the number of agents in parallel.
We have two solutions to solve this problem: (a) Compute all trajectories asynchronously or (b) compute all guide paths synchronously and perform P2 asynchronously.

Using a master-worker architecture, we implemented both solutions (a) and (b) that can be seen on figure \ref{fig:architecture_master_worker}.
\begin{figure}
    \captionsetup[subfigure]{justification=centering}
    %\centering
    \begin{subfigure}[t]{\linewidth}
    \includegraphics[width=\textwidth]{Figures/Chapter_LEAS/architecture_master_worker_0.png}
    \caption{Asynchronous guide path and contact planning}
    \end{subfigure}
    \begin{subfigure}[t]{\linewidth}
    \includegraphics[width=\textwidth]{Figures/Chapter_LEAS/architecture_master_worker_1.png}
    \caption{Synchronous guide path and Asynchronous contact planning}
    \label{fig:architecture_master_worker_asynchronous}
    \end{subfigure}
    \caption{Two versions of Master-Workers architecture to learn LEAS plugged to a contact planner.}
    \label{fig:architecture_master_worker}
\end{figure}

% Solution A : all asynchronous
Solution (a) dissociates the learning operated by the master and the trajectory generation performed by the workers (so-called distributed RL).
The advantages of this method are its scalability as we can add during the learning as many agents as desired that can run on different machines, and its modularity as the agents can be initialized on new terrain and stopped if needed.
This method is used in IMPALA \cite{impala2018} running 32 CPUs and RAPID \cite{openai2019dota} with 500 CPUs, showing impressive results to learn very challenging and high complexity tasks.
However, this method also requires a lot of data exchanges between the RL algorithm and the workers (asynchronous agents) as expressed in \cite{deepRLvsES}. 
The load on the network increases with the number of data (the trajectories) exchanged between the master and the workers, plus the parameters (policy network) that must be shared with all the workers after each update.
As a consequence, this can result in some hardware and implementation limitations depending on the available resources, and a possible delay in the policy of the asynchronous agents.

% Solution B : Guide path synchronous and contact planning asynchronous
The other solution (b) lets the master generate all the trajectories with one or several synchronous agents, then dispatch the guide paths generated to the workers that compute P2 and return the result to the client.
It is a solution specifically adapted to our problem and simple to implement that can be added on top of any on-policy or off-policy RL algorithm. 
One or several agents generate synchronously some guide paths, that are shared with external workers that compute the contact sequence. Then, they return their feedback to the master for the learning. 
The advantage of this method is the reduction of data exchanges between the master and the workers. Only the guide path, a sequence of positions and orientations, and the feedback from the contact planner transit on the network.
However, if the guide path generation P1 is faster than P2, we can see some lags between the synchronous agents and the workers. That is why we need to balance their number or add more workers to follow the flow. Also, the parallel agents during the learning run on environments that can not be changed manually, making it a simple but less modular method overall compared to (a).

In our experiments, solution (a) led to a bottleneck on our hardware due to the amount of data exchanged, and going for pre-made implementation as IMPALA was not necessary for our problem that only requires a few hours of training compared to the tasks solved in \cite{impala2018, openai2019dota}.
Therefore we opted for solution (b) which is a good compromise between synchronous and asynchronous steps.
In the future, we would like to take a look at fully decentralized architectures \cite{DD_PPO} that solve the problem of network overload by sharing only the gradients for training.

With setup (b), we perform several tests on basic scenarios with LEAS without contact using the original PPO algorithm, then plug it into a contact planner using our modified PPO version to analyze its impact on the trained policy.

% Say that for the results bellow, we do not use the contact planners, equivalent to a contact planner that returns that all trajectories are succesful and returns last index of obs.

\section{Results\label{subsec:leas-results}}

\begin{figure}
    \centering
    \includegraphics[width=0.6\textwidth]{Figures/Chapter_LEAS/learning_curves_P1.png}
    %\begin{subfigure}[t]{0.49\linewidth}
    %\includegraphics[width=\textwidth]{Figures/Chapter_LEAS/learning_curves_P1.png}
    %\caption{}
    %\end{subfigure}
    %\begin{subfigure}[t]{0.49\linewidth}
    %\includegraphics[width=\textwidth]{Figures/Chapter_LEAS/learning_curve_leas_p1.png}
    %\caption{}
    %\end{subfigure}
    \caption{Learning curves of LEAS without contact planner feedback (pure navigation task) with: (a) Control in acceleration (Orange) with delay, (Blue) without delay, on the position update, and (b) the complete learning curve without delay.}
    \label{fig:control_LEAS_learning_curves}
\end{figure}

\begin{table}[tb]
\begin{center}
\caption{Parameters}
\begin{tabular}{|c|c| c |c|c|}
 State & $81$ && Max Episode Length & $800$\\
 Actions & $4$ && Parallel agents & $6$\\
 $a_{max}$ & $0.08$ m/s$^2$ && Workers & $6+$\\
 $v_{max}$ & $0.2$ m/s && Batch size & $4096*M$\\
 $v_{desired}$ & $0.1$ m/s && Mini-Batch size & $256$\\
 $\omega_{max}$ & $\pi/9$ rad/s && Learning rate & $[5e-4,1e-5]$\\
 Timestep $T$ & $0.2$ s && Noptepochs & $10$\\
 Local heightmap $H$ & 10x14 && Discount Factor ($\gamma$) & $0.97$\\
 $z_{max}$ & $-0.2$ m  && Clip range & $0.2$\\
 $z_{min}$ & $-1.4$ m &&
 $w_{ori}$ & $0.4$ \\
 $w_{dir}$ & $1.0$  &&
 $w_{acc}$ & $0.1$ \\
 $w_{\omega}$ & $0.1$  &&
 $w_{alive}$ & $1.0$
\end{tabular}
\label{tab:param}
\end{center}
\end{table}

\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{Figures/Chapter_LEAS/arena_5x30.png}
    \caption{Training terrain of LEAS: a 5x30 arena (corresponding to 10x60 meters) composed of 86 links: 17 bridges, 31 stairs, 8 rubbles and 30 flat ground with obstacles. All links are two-way and have different elements characteristics and slopes.}
    \label{fig:arena_5x30}
\end{figure}

% Explain what do we use
We used the HPP software \cite{HPP_software} and the humanoid robot Talos model \cite{talos_robot}. Our algorithm was implemented in python using the PPO implementation of Stable Baselines \cite{stable-baselines} modified for our Master-Workers architecture.
% Parameters
All parameters in the environment and hyperparameters to control the learning process can be seen on Table \ref{tab:param} and we use the terrain generator described in \ref{subsub:terrain_generator} to generate the 5x30 terrain on figure \ref{fig:arena_5x30}.

% Explain parameters for the ENV
% - timestep => Why this value
We set a fixed timestep value of $0.2$ seconds, which for $v_{max}=0.2$ m/s corresponds to a maximum distance between each state on the path of 4 centimeters, and 2 centimeters for $v_{desired}=0.1$ m/s.
% - number of steps max => Important to explore enough, not do only links and not be stuck somewhere.
Each episode has a maximum length of $800$ steps, meaning that the guide path can contain up to $800$ states (a maximum distance of 32 meters). During the training, the first goal is set such that the robot has to cross a link first (stairs, rubbles, or bridge), then we uniformly sample a new goal direction every $n_{rand} \in [200,800]$ steps. The first goal prioritizes the task of crossing a link that we will perform in all our basic scenarios, while the others often lead to a dead-end or a difficult path where the agent will have to learn if it has to stop or if it has the ability to cross it.

% Param for learning
We linearly decay the learning rate from $5e-4$ to $1e-5$ over 2 millions steps and set a discount factor $\gamma = 0.97$. 
A method to tune the discount factor is to calculate the half-life $\tau = \frac{1}{1-\gamma}$ which roughly corresponds to the number of steps considered to adapt the agent behaviour, and is equal for LEAS to $\tau = \frac{1}{1-0.97} \approx 33$ steps. 
As a result, steps from $[0,33]$, $[33,66]$, $[66,99]$, $[99,\infty]$ will roughly account for $63\%$, $23\%$, $8\%$ and $6\%$ of the sum of discounted rewards respectively. 
The distance traveled by the RL agent after 33 steps is equal to 66 cm and 132 cm for velocity equals to $v_{desired}$ and $v_{max}$ respectively.
% Notes: integral 0 to inf (0.97**x) = 32.8  => from 0 to 33=20.8 => from 33 to 66=7.6 => from 66 to 99 = 2.78. So most of the reward comes from 0 to 33 (64%), from 33 to 66 (23%), from 66 to 99=8%. If discount factor 0.98 : half-life at 50 => 0-50:64% => 50-100:23% => 100-150=8%
We emphasize that we want to learn a steering method to navigate locally and that the agent only needs to think about its very near future so that the detection of an obstacle should impact its action only when close to the robot (distance inferior to 1 meter).

% Number of workers
The number of workers performing the contact planning phase is set to 0 when we train LEAS for a pure navigation task, and we increase this number when a contact planner is used during the learning. In our experiment, the number of asynchronous workers is limited to 6 due to the limit of available CPU cores on our hardware.

% Explain choices for the RL parameters used:
% - size of network with PPO : 256x128x64
The PPO actor and critic are two distinct networks with hidden layers of size 128x64x32. Tuning the number of nodes and hidden layers in the machine is empirical and depends on the task to perform. In general, deeper networks with more nodes means better capability to solve very complex tasks and less under-fitting. However, it also means more parameters to train and can be prone to over-fitting. In our experiments on LEAS, we found no noticeable difference between 2 or 3 hidden layers and nodes number ranging from 64-256. However, we increased the network size of 64x64 from the first version of LEAS \cite{LEAS} for more potential scalability in our tests in terms of observations and contact planners.

We train LEAS without a contact planner and evaluate the model after 5 million steps corresponding to around 1 hour of training on a PC with an Intel Core i7-8700 (12 cores, 3.20Ghz, 16GB ram). Learning curves can be seen on figure \ref{fig:control_LEAS_learning_curves}.

\subsection{Comparison: design\label{tab:compare_sm_charac}}

\begin{center}
\begin{tabular}{ |c|c|c|c| }
\hline
Steering Method & RB-Lin & RB-Kino & LEAS (ours)\\
\hline
Goal Connection & 
\thead{\textcolor{red}{Exact}\\pos,ori} & 
\thead{\textcolor{red}{Exact}\\pos,vel,acc,ori}  & 
\thead{\textcolor{blue}{Near}\\pos}
\\
\hline
Dynamic constraints &
\thead{\textcolor{red}{1}\\$v_{max}$} &
\thead{\textcolor{red}{2}\\$v_{max},a_{max}$} &
\thead{\textcolor{blue}{3}\\$v_{max},a_{max},\omega_{max}$}
\\
\hline
Terrain aware &
\thead{\textcolor{red}{No}} &
\thead{\textcolor{red}{No}} &
\thead{\textcolor{blue}{Yes}}
\\
\hline
\thead{Path planning\\dependant} &
\thead{\textcolor{red}{Yes}} &
\thead{\textcolor{red}{Yes}} &
\thead{\textcolor{blue}{No}}
\\
\hline
\end{tabular}
\end{center}

% Compare it to SM : RB-Kino and RB-Lin
% Show the difference between all SM :
% - RB-Kino connects exactly two points and its shape is relative to the two configs linked. 
% Advantage: connect exactly and we can control the acceleration and velocity at the goal + control on the acceleration.
% Disadvantage: Requires path planning to find the acc/vel/orientation at the arrival to obtain a valid guide path. Totally dependant on the path planning + can not be used for a control with a remote + does not consider the terrain to generate the trajectory.
% - RB-Lin connects exactly two points and do not perform DIMT, it's just a linear interpolation in velocity, orientation and position. No control on the acceleration.
% Advantage: Very simple to implement.
% Disadvantage: Also requires path planning as a line guide path is not sufficient very often to climb stairs + does not consider the terrain to generate the trajectory.
% - LEAS does not connect exactly two points and will just end up near the goal position.
% Advantage: Less dependant on path planning as the goal state is smaller (only position) and it can locally navigate depending on the terrain + control on the acceleration max.
% Disadvantage: No control on the velocity and orientation at the goal. Does not end exactly on the goal.
We compare LEAS, a flexible alternative to our previous steering methods where their characteristics can be seen in Table \ref{tab:compare_sm_charac}.

% Goal connection
\noindent \textbf{Goal connection}:
RB-Kino \cite{kinodynamic-sm} and RB-Lin \cite{AcyclicCP} are two steering methods that connect exactly some initial and goal configurations in position, velocity, and orientation. 
% RB-Kino
RB-Kino uses the Double Integrator Minimum Time \cite{DIMT_kino_planning_manipulator} to also add a constraint on the acceleration.
% LEAS
LEAS does not exactly connect the initial and goal configurations as it is trained to follow a given goal direction, and as a result, LEAS can at best end up near the goal position.
The capability to connect exactly two configurations is needed in robotics for hand manipulation \textcolor{red}{Cite papers where they need exact connection with DIMT}. However, in locomotion, this accuracy is not required in most scenarios.
This is especially true on long paths with key waypoints to sequentially reach up to a distant objective, where passing close enough to each waypoint is sufficient.
As LEAS uses a target direction instead of a target configuration, this also allows controlling LEAS using a remote controller, which is equivalent to a very distant target that the agent tries to reach. We simulate this kind of control for the training of LEAS by randomly repositioning its goal.

% Dynamic constraints
\noindent \textbf{Dynamic constraints}:
% RB-Lin constraints
RB-Lin is a modified linear interpolation that is constrained on the orientation and velocity, as it first rotates the robot toward the target at $\omega_{max}$, then moves it to the goal at $v_{desired}$.
% RB-Kino constraints
RB-Kino requires two parameters $v_{max}$ and $a_{max}$ that act as strict constraints on both velocity and acceleration. However, no constraints are set on the angular velocity $\omega_{max}$ and this can be a problem as we will see on chapter \ref{sec:CP-SB}. \textcolor{red}{TO CHECK, no constraint on angular velocity ? Why Pierre did not change it ? He prefered starting with a higher init velocity to avoid this problem.}
% LEAS constraints
On the other hand, the control of LEAS constrains each configuration with $v_{max}$,$a_{max}$,$\omega_{max}$. 
In this thesis, we use only a quasi-static contact planner so $v_{max}$ and $a_{max}$ are considered during the guide path planning but not the contact planning phase. However, using LEAS with a kinodynamic contact planner is a possibility.

% Terrain aware + path planning
\noindent \textbf{Terrain-aware and path planning}:
RB-Kino and RB-Lin both require a planning algorithm (section \ref{subsub:problematic_leas}) to place additional waypoints to reach a distant goal. In contrast, LEAS can visualize its surrounding terrain and is able to locally navigate in one direction thus removing the need for path planning on basic scenarios (i.e. crossing a transition link). 
In complex scenarios, path planning algorithms can be used to place waypoints followed by LEAS. 
In this work, LEAS can navigate the same waypoints computed by the RRT with the RB-Kino \cite{kinodynamic-sm}, and having a RRT with LEAS in the same manner as RL-RRT \cite{RL_RRT} is a work in progress.

Finally, we recall the main advantage of LEAS over our previous steering methods which is the use of Reinforcement learning, that through the trajectory validation with the contact planner can alter its behaviour to fit it.

\subsection{Test scenarios\label{subsub:leas:test_scenarios}}

\textcolor{red}{\textbf{TO DO}: Show a pic of all the scenarios in gepetto viewer.}

As LEAS does not connect exactly with the goal position, we consider the goal as reached when the distance from the last state on the guide path to the goal is lower than a distance threshold $\epsilon$ set to $20$ cm for all our scenarios.

% Show behaviour:
% [0] Navigate while keeping a desired velocity => Show acceleration, velocity, orientation, reward.
% Start from a -pi orientation.
% Show it on Ground.

% [1] Stop when there is an obstacle.
\begin{figure}
    \centering
    \captionsetup[subfigure]{justification=centering}
    \begin{subfigure}[t]{0.43\linewidth}
    \includegraphics[width=\textwidth,height=6cm]{Figures/Chapter_LEAS/stop_bauzil_0.png}
    \end{subfigure}
    \begin{subfigure}[t]{0.43\linewidth}
    \includegraphics[width=\textwidth,height=6cm]{Figures/Chapter_LEAS/stop_bauzil_1.png}
    \end{subfigure}
    \label{fig:leas_stop_void_obstacle}
    \caption{LEAS detects an obstacle and lower its velocity to avoid any unvalid state: collision or ground not reachable (robot contact configurations computed with SB-CP for a better visualization).}
\end{figure}

% [2] On stairs, compare with RB-Kino/RB-Lin, the picture with the points, success SM.
\begin{figure}
    \centering
    \captionsetup[subfigure]{justification=centering}
    \begin{subfigure}[t]{0.4\linewidth}
    \includegraphics[width=\textwidth, height=5cm]{Figures/Chapter_LEAS/stairs_exemple.png}
    \caption{Scene view}
    \end{subfigure}
    \begin{subfigure}[t]{0.49\linewidth}
    \includegraphics[width=\textwidth, height=5cm]{Figures/Chapter_LEAS/stairs_lin_p1_90.png}
    \caption{RB-Lin}
    \end{subfigure}
    \begin{subfigure}[t]{0.49\linewidth}
    \includegraphics[width=\textwidth, height=5cm]{Figures/Chapter_LEAS/stairs_kino_p1_90.png}
    \caption{RB-Kino}
    \end{subfigure}
    \begin{subfigure}[t]{0.49\linewidth}
    \includegraphics[width=\textwidth, height=5cm]{Figures/Chapter_LEAS/stairs_leas_p1_90.png}
    \caption{LEAS (ours)}
    \end{subfigure}
    \caption{Comparison on stairs where dots represent initial states from where the SM generates a valid (yellow) or unvalid (black) guide path up to the goal (red). We illustrate for each SM one trajectory where the black arrows represent the robot orientation along the guide.}
    \label{fig:stairs_p1}
\end{figure}

\textbf{Stairs}: We compare LEAS to our previous steering methods on the stairs scenario as shown on figure \ref{fig:stairs_p1}. The heightmap of the terrain is represented by the grey shades with the bottom of the stairs on the left and the top of the stairs on the right. We uniformly sample 80 initial positions at the bottom of the stairs from where the SM has to generate a guide path up to the same fixed goal located at the top. Each initial state is oriented at $90^{\circ}$ to better show the rotation performed by the steering methods and its initial velocity is set to $0$ m/s. 
Initial states from where the SM reaches the goal with a valid guide path are represented by a yellow dot, and a black dot represents a state where either the SM fails to reach the goal or generates an invalid guide path (i.e. ground not reachable or collision) and thus requires path planning to succeed. 
For RB-Kino, we need to define additional constraints that are the velocity and acceleration desired on the goal configuration and that have an impact on the shape of the trajectory. In this scenario, we set the desired goal acceleration and velocity to $0$.

RB-Lin (a) is designed such that it first rotates the robot toward the goal, then generates a straight line up to the goal. The main limitation of this method is that it succeeds only when directly in front of the stairs and further initial position on the left results in guide paths where the robot is unable to touch the ground.
RB-Kino (b) presents the same limitation as RB-Lin and an additional one on the orientation where it rotates directly from $90^{\circ}$ to $0^{\circ}$ in one timestep. This is a problem inherent to RB-Kino which is the correlation between the initial velocity and the angular velocity. As a consequence starting with null velocity, as we will see later on, is critical with our contact planners as such fast rotation is not possible kinematically. 
In practice, we avoid this problem by adopting the same strategy as RB-Lin by first rotating the robot and as result, we always start RB-Kino with a $0^{\circ}$ orientation.

LEAS (c) succeeds on a much broader range of initial states, removing the need for path planning in most cases. Our steering method has to generalize to this scenario that has never been encountered during its training: it smoothly rotates toward the goal, detects the stairs at its local heightmap and adapts its velocity $v_z$ to climb the stairs while keeping a valid state, finally reaching an area of $20$ cm around the goal.

% [2] On hole, compare with RB-Kino/RB-Lin, the picture with the points, success SM.
\begin{figure}
    \centering
    \captionsetup[subfigure]{justification=centering}
    \begin{subfigure}[t]{0.49\linewidth}
    \includegraphics[width=\textwidth]{Figures/Chapter_LEAS/hole_p1_lin.png}
    \caption{RB-Lin}
    \end{subfigure}
    \begin{subfigure}[t]{0.49\linewidth}
    \includegraphics[width=\textwidth]{Figures/Chapter_LEAS/hole_p1_kino.png}
    \caption{RB-Kino}
    \end{subfigure}
    \begin{subfigure}[t]{0.49\linewidth}
    \includegraphics[width=\textwidth]{Figures/Chapter_LEAS/hole_p1_leas.png}
    \caption{LEAS (ours)}
    \end{subfigure}
    \begin{subfigure}[t]{0.49\linewidth}
    \includegraphics[width=\textwidth]{Figures/Chapter_LEAS/hole_p1_kino_rrt.png}
    \caption{RB-Kino + RRT}
    \label{fig:leas:hole_scenarios_Kino_RRT}
    \end{subfigure}
    \caption{Comparison of steering methods on hole scenario where dots are initial states from where the SM generates a valid (yellow) or invalid guide path (black) invalid guide path up to the goal (red). Guide paths are represented by the blue trajectories.}
    \label{fig:leas:hole_scenarios}
\end{figure}

\textbf{Hole}: A comparison on the hole scenario is shown on Figure \ref{fig:leas:hole_scenarios}. Each initial configuration has a null velocity and is directly oriented toward the goal located on the other side of the hole. 
RB-Kino and RB-Lin generate valid guide paths only when the problem is feasible in a straight line and so require path planning to succeed in some of the initial configurations on this scenario.
In contrast, LEAS can reach the goal as long as it detects the surface to the left of the hole on its local heightmap, which we recall has a scope of 110 cm in front and 60 cm on each side of the robot. 
On most of the difficult configurations on the right, LEAS detects the hole and slowly slides along it keeping a valid state until the surface on the left is detected.
% Still above the void
We notice that some configurations still lie above the hole less than 10 centimeters far from the surfaces and a tighter constraint on the heightmap values under the robot as expressed in \ref{subsub:validity_approximation} can be set to avoid it. However, the Talos robot is 55 cm in width meaning that most of its body still lies above the ground.

% Hole kino and path planning
Finally, Figure \ref{fig:leas:hole_scenarios_Kino_RRT} illustrates RB-Kino combined with an RRT path planning algorithm to solve the hole scenario. To generate these trajectories, we use the full validity condition without the stricter constraint $\mathcal{R}$ (defined in section \ref{fig:approximation_validity}). As a result, most guide paths generated lie above the hole with a maximum distance of 40cm from the surfaces, corresponding to the width size of the range of motion of the legs. 
This brings out the main problem of our previous validity condition and the need to have a stricter constraint to avoid such configurations on the guide path.

%\textcolor{red}{TALK MORE ABOUT PATH PLANNING AND WHY WE DONT WANT TO USE IT HERE : 1.45s to compute it with bi-rrt on this scenario, several seconds on scenarios in their previous paper in C++. In comparison LEAS in python with my non-optimized code takes 0.56s in average on this scenario. => I can use the wall scenario to talk about that + need to add computation time on the hole scenario for Kino RRT}


% In general how much percent success in our scenarios ?
% -- On 1x11 with initial orientation of -180 degrees:
% Rubbles => 100/100
% Bridge => 31/100
% Stairs down => 100/100
% Stairs up => 86/100
% -- On 1x11 with initial orientation of 0 degrees:
% Rubbles => 100/100
% Bridge => 76/100
% Stairs down => 100/100
% Stairs up => 100/100
\begin{figure}[h!]
    \centering
    \includegraphics[width=\textwidth]{Figures/Chapter_LEAS/1x11_tests.png}
    \includegraphics[width=\textwidth]{Figures/Chapter_LEAS/1x11_example_180deg.png}
    \caption{Terrain to evaluate our steering methods success rate, with 4 transition tiles: rubbles, bridge, stairs (down), and stairs (up). Examples of extreme initial configurations (dots) from where the SM generates: a valid (yellow) or invalid guide path (black) up to a goal (red). Black arrows are the root orientation along the guide (blue).}
    \label{fig:tests_1x11}
\end{figure}

\begin{table}[h!]
\centering
\begin{tabular}{ |c|c|c|c|c| } 
    \hline
    Initial orientation & Rubbles & Bridge & Stairs (down) & Stairs (up) \\ 
    \hline
    $0^{\circ}$ & 100 \% & 100 \% & 100 \% & 100 \%  \\ 
    \hline
    $180^{\circ}$ & 100 \% & 87 \% & 100 \% & 97 \% \\
    \hline
\end{tabular}
\caption{Success rate of LEAS with two initial orientations on 100 uniformly sampled trajectories for each terrain of Figure \ref{fig:tests_1x11}. The initial position can be far or very near the obstacle, letting enough time to LEAS to rotate the robot or not and impacting the success rate.}
\label{tab:tests_1x11}
\end{table}

\textbf{Evaluation of LEAS}: We evaluate the success of LEAS (Table \ref{tab:tests_1x11}) to reach a goal position on our evaluation terrain never met during its training (Figure \ref{fig:tests_1x11}).
%On this terrain, the width of the bridge, the orientation of the rubbles and the size of the stairs differ from transition tiles encountered in the 5x30 training ground, forcing LEAS to generalize to this new environment. 
We uniformly sample 100 configurations before each transition tile to cross and test two initial orientations of $0^{\circ}$ and $180^{\circ}$ to evaluate their impact on the success of LEAS. 

For both rubbles and stairs (down), LEAS succeeds in all 100 trajectories whatever its orientation. 
However, we observe some scenarios where our steering method fails, mainly from initial configurations facing backward and very close to the obstacle to cross. To succeed, the agent has to rotate the robot to have a clear vision of the obstacle in its back in order to cross it. 
These difficult cases appear on the bridge where the agent stops the robot near the void to avoid falling and on the stairs up where the agent collides with the stairs during its rotation.

\begin{figure}
    \centering
    \includegraphics[width=0.6\textwidth, height=6cm]{Figures/Chapter_LEAS/follow_waypoints_bauzil.png}
    \caption{LEAS following manually placed waypoints (yellow cross) to reach a distant goal.}
    \label{fig:bauzil_waypoints}
\end{figure}
Finally we manually place some waypoints to reach a distant goal on a complex scenario as seen on Figure \ref{fig:bauzil_waypoints}. LEAS successfully reaches each waypoint navigating across this terrain composed of stairs and a bridge.



\section{Discussion\label{sub:leas:discussion}}
% Say that it works far better than before and offer a good compromise compared to our previous methods.
LEAS learned by reinforcement offers a flexible alternative to our previous steering methods. Our results show that LEAS performs better in generating valid guide paths than RB-Kino and RB-Lin thanks to its local terrain-awareness strategy, succeeding in a broader range of our test scenarios and permitting more methods of user control by providing a goal direction instead of a goal configuration. However, LEAS presents some limitations in its design and further improvements will be required in the future.

\subsection{Limitations}
% FAILURE CASES
% Fail often on the bridge
\textbf{Terrain visualization}: 
Parameters $z_{min}$, $z_{max}$ and the heightmap resolution impacts the capabilities of the agent to detect its environment.
In our experiments, the most difficult scenarios to perform with LEAS were the crossing bridge and avoiding obstacles where the tuning of these values was critical.
First, a too low value $z_{min}$ can lead LEAS to interpret the void surrounding the bridge as the next step of some stairs. 
This caused LEAS to lower the root of the robot at the limit of the collision with the trunk to further evaluate if what it sees is really the void or some stairs. 
As a consequence, this led to some possible collisions with the bridge with these two conflicting behaviours.
A similar problem appeared for a too high value $z_max$, with the interpretation of an obstacle as the next step of some stairs and causing a collision with it.
We finally found values $z_{min}$ and $z_{max}$ avoiding such cases while correctly representing small height variations.

% Play on R_ALIVE
To avoid such collisions, one could increase $w_{alive}$ to encourage the robot to act more carefully (discussed in section \ref{subsubsec:states}). 
We tested this solution that greatly improve the capabilities of LEAS for collision avoidance and keeping a reachable state at all times, however, LEAS was acting so safely that it did not dare to cross difficult transitions tiles, making the robot stay idle in front of the bridge to safely accumulate the reward $R_{alive}$. 
The final value $w_{alive}=1$ decided for LEAS offers a compromise safety/risk but requires further investigation. 

% RL so the SM is not perfect and depends on many things
\textbf{Constraint on the orientation}: 
%Continuous agent learned by Reinforcement can still fail in performing their task, as LEAS due to a collision or staying idle instead of moving forward.
%An explanation of why the LEAS can present atypical behaviour on some scenarios, why it does not succeed in generalizing or it generates guide path that are not straight is complex to analyze. It depends on the reward design, the control and the terrain where the agent has been trained on. 
% Exploration with orientation behaviour
We observed that LEAS trained on a complex terrain will have a tendency to explore more its surrounding, orient its root and so its local heightmap to have a better vision of it. 
We limit such behaviour by setting a low discount factor $\gamma$ and a high weight $w_{ori}$ to enforce its orientation toward the goal. 
However such a reward will probably not work for cluttered environments requiring the robot to sidewalk and so requires further investigation.

% Accuracy of LEAS
\begin{figure}
    \centering
    \includegraphics[width=0.6\textwidth, height=6cm]{Figures/Chapter_LEAS/test_epsilon.png}
    \caption{Average number of steps required to reach an area of radius $\epsilon$ cm around the goal.}
    \label{fig:nb_steps_required}
\end{figure}\
\textcolor{red}{Should I keep this figure to illustrate the text bellow ? The picture is not that interesting maybe.}

\textbf{Goal accuracy}: \textcolor{red}{Other idea of name ?} 
We recall one of the main limitations compared to our previous steering methods which is the non-exact connection to the goal position. We consider the goal as reached if the distance between the state on the guide path and the goal is inferior to a threshold of value $\epsilon$. In all our scenarios, we fix $\epsilon = 20$ cm that we consider accurate enough.
We further evaluate the accuracy of LEAS to reach a goal position in Figure \ref{fig:nb_steps_required}. For this test, we set the robot orientation back to the transition tile (rubbles for this scenario) and we uniformly sampled 50 configurations. The task to perform is to rotate the robot toward the fixed goal on the other side of the rubbles and get close to it, less than $\epsilon$ cm.
We test values of $\epsilon \in [1.5, 20]$ cm. 
For all $\epsilon \geq 1.5$ cm, all 50 trajectories reach the goal under the threshold. However, we can observe that as the value of epsilon decreases, the number of steps to reach the goal increases where the agent passes close by but misses the area of radius $\epsilon$ around the goal and has to move back and forth to reach such accuracy. 
In this test scenario, LEAS reaches the goal under the tested threshold at once for all threshold $\geq 7.5 $ cm. In all our test scenarios, we set $\epsilon=20$ cm to have a sufficient margin of error.

% Difficult to define good bounds for the heightmap to best represent small and high height variations. Using a second heightmap is a possibility but it increases the number of states and we want to avoid it. We could use a non linear, as a square function on values of the relative heightmap to better capture small variations near the robot's feet while ensuring height elevation detection for stairs, obstacles and void.
%\textbf{Surface detection}: 
\subsection{Future improvements}

For a pure navigation task, LEAS without a contact planner does not need to clearly identify the surfaces of the terrain and solely focus on state validity from the heightmap. 
However, in the context of contact planning, LEAS may require to identify these surfaces to step.
In this work we use a simple multilayer perceptron, as done in \cite{RL_RRT, RL_RRT_AUTORL} with 1-D lidar values, that is simple to implement and fast to train for the navigation task. 
While this terrain representation is sufficient for the task performed in chapter \ref{sec:LEAS} and \ref{sec:CP-SB}, we believe that contact planners of chapter \ref{sec:CP-SL1M} could greatly benefits from convolutional neural network architecture to extract features from the heightmap and learn a better representation of the terrain \cite{deepLoco,deepGait,RLOC}.

The diversity of terrains was sufficient to train LEAS for our locomotion task, but more terrains could improve its generalization capabilities. This would require another architecture than our actual Master-Workers for a fully asynchronous one \cite{impala2018, DD_PPO}, and another simulation environment as \textit{RaiSim} \cite{raisim}, allowing us to switch terrain efficiently along with its heightmap.

% How can we improve LEAS: As stated before, better terrains as grid terrains are still limited, biggers but with HPP not possible for now, more methods to have a better generalization with noise 
% + mirroring: it would be smart to mirror all the states and actions to generate twice as much datas. This would also get us a symmetric behaviour that we do not have for now.
%\textbf{Data augmentation}: 
Finally, we discussed about curriculum learning \cite{curriculum_learning_survey} to incrementally increase the complexity of the tasks to solve during the training, which did not improve the result of LEAS. But several methods in the literature could improve its learning efficiency and overall performance.
Especially mirroring all states relative to the robot orientation axis are other strategies to explore and that could greatly improve the sample efficiency during the training and obtain a policy with a symmetric behaviour.


\subsection{Conclusion}
% Recall that this is a prototype, but that is still better than our previous steering methods and in this paper we are aiming to see if a high level approach i.e. with a guide path can improve the performances of the contact planners.
% Conclusion
We presented LEAS, a RL steering method to locally navigate on uneven terrains and to generate a guide path subject to reachability and collision avoidance constraints. 
Such terrain-aware steering methods remove the need for a path planning algorithm, expensive to compute, in most of our scenarios.
As a result, LEAS can directly be integrated as the module P1 of our locomotion pipeline (Figure \ref{fig:pipeline}).
However we still do not solve the lack of synergy between P1 and the contact planner P2, and that is why in the next chapters we answer the following question: can LEAS navigate on complex terrains while generating a guide path improving the success rate of a given contact planner?
